{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4162dbbe-ceb6-47f9-912f-f8f630017913",
   "metadata": {},
   "source": [
    "# Langchain å¤šä»»åŠ¡åº”ç”¨å¼€å‘\n",
    "\n",
    "# å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChain: ä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶  \n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨  \n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡\n",
    "\n",
    "# å†™åœ¨å‰é¢ï¼š\n",
    "\n",
    "- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ (SDK)  \n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹  \n",
    "- å­¦ä¹  LangChain è¦å…³æ³¨æ¥å£å˜æ›´\n",
    "\n",
    "# LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "- Chat Models: å¯¹è¯­è¨€æ¨¡å‹æ¥å£çš„å°è£…  \n",
    "- PromptTemple: æç¤ºè¯æ¨¡æ¿  \n",
    "- OutputParser: è§£æè¾“å‡º\n",
    "\n",
    "2. æ•°æ®è¿æ¥å°è£…ï¼ˆå¼±äº LlamalIndexï¼‰\n",
    "\n",
    "- Document Loaders: å„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨  \n",
    "- Document Transformers: å¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc  \n",
    "- Text Embedding Models: æ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œ  \n",
    "- Verctorstores & Retrievers: å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n",
    "\n",
    "# 3. æ¶æ„å°è£…\n",
    "\n",
    "- Chain/LCEL: å®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ  \n",
    "- Agent: æ ¹æ®ç”¨æˆ·è¾“å…¥,è‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤,è‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·,æœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½  \n",
    "- Tools: è°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°,ä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰  \n",
    "- LangGraph: å·¥ä½œæµå¼€å‘æ¡†æ¶  \n",
    "4. LangSmith: è¿‡ç¨‹ç›‘æ§ä¸è°ƒè¯•æ¡†æ¶\n",
    "  \n",
    "![](./img/1.png)\n",
    "\n",
    "# æ–‡æ¡£ï¼ˆä»¥Pythonç‰ˆä¸ºä¾‹ï¼‰\n",
    "\n",
    "- åŠŸèƒ½æ¨¡å—ï¼šhttps://python.langchain.com/docs/tutorials  \n",
    "- API æ–‡æ¡£: https://python.langchain.com/api_reference/  \n",
    "- ä¸‰æ–¹ç»„ä»¶é›†æˆï¼šhttps://python.langchain.com/docs/integrations/provider/  \n",
    "- æ›´å¤š HowTo: https://python.langchain.com/docs/how_to/\n",
    "\n",
    "# LangChain æ˜¯å¼€æºé¡¹ç›®\n",
    "\n",
    "é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai\n",
    "\n",
    "# 1. æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "æŠŠä¸åŒçš„æ¨¡å‹,ç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£,æ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚  \n",
    "## 1.1 æ¨¡å‹ API: ChatModel  \n",
    "### 1.1.1 OpenAIæ¨¡å‹å°è£…\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff32b7f0-e725-4856-885e-826688995717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain\n",
    "#!pip install -U langchain-openai\n",
    "#!pip install openai\n",
    "#!pip install langchain-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e41e9f-2459-47f4-81ba-912de7ae46d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={} client=<openai.resources.chat.completions.completions.Completions object at 0x1107e2950> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x110da6b90> root_client=<openai.OpenAI object at 0x1103e87d0> root_async_client=<openai.AsyncOpenAI object at 0x1103e9f50> model_name='deepseek-chat' model_kwargs={} openai_api_key=SecretStr('**********') api_key=SecretStr('**********') api_base='https://api.deepseek.com/v1'\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ï¼ğŸ˜Š\n",
      "\n",
      "æˆ‘æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æ¨¡å‹ï¼Œè™½ç„¶ä¸æ”¯æŒå¤šæ¨¡æ€è¯†åˆ«åŠŸèƒ½ï¼Œä½†æˆ‘æœ‰æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œå¯ä»¥å¸®ä½ å¤„ç†å›¾åƒã€txtã€pdfã€pptã€wordã€excelç­‰å„ç§æ–‡ä»¶ï¼Œä»ä¸­è¯»å–æ–‡å­—ä¿¡æ¯è¿›è¡Œåˆ†æå¤„ç†ã€‚æˆ‘å®Œå…¨å…è´¹ä½¿ç”¨ï¼Œæ‹¥æœ‰128Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿˜æ”¯æŒè”ç½‘æœç´¢åŠŸèƒ½ï¼ˆéœ€è¦ä½ åœ¨Web/Appä¸­æ‰‹åŠ¨ç‚¹å¼€è”ç½‘æœç´¢æŒ‰é”®ï¼‰ã€‚\n",
      "\n",
      "ä½ å¯ä»¥é€šè¿‡å®˜æ–¹åº”ç”¨å•†åº—ä¸‹è½½æˆ‘çš„Appæ¥ä½¿ç”¨æˆ‘ã€‚æˆ‘å¾ˆä¹æ„ä¸ºä½ è§£ç­”é—®é¢˜ã€ååŠ©å¤„ç†æ–‡æ¡£ã€è¿›è¡Œå¯¹è¯äº¤æµç­‰ç­‰ï¼\n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯æ—¥å¸¸é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆæ„¿æ„ä¸ºä½ æä¾›å¸®åŠ©ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model('deepseek-chat',model_provider='deepseek',\n",
    "        api_key= \"sk-fa1e532719fc46aa83166d15619bb76a\"\n",
    "    )\n",
    "print(model)\n",
    "response = model.invoke('ä½ æ˜¯è°')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e56d37-b47a-4693-b574-c769d6cabda7",
   "metadata": {},
   "source": [
    "###  1.1.2 å¤šè½®å¯¹è¯Seesion å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4cd1cf0-5f7b-40cd-9dc7-5383d5b5b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ lojzesï¼Œæ˜¯ lojzes å¤§æ¨¡å‹è¯¾ç¨‹çš„å­¦å‘˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import(\n",
    "    AIMessage, # ç­‰ä»·äº OpenAI æ¥å£ä¸­çš„ assisant role\n",
    "    HumanMessage, # ç­‰ä»·äº OpenAI æ¥å£ä¸­çš„ user role\n",
    "    SystemMessage # ç­‰ä»·äº OpenAI æ¥å£ä¸­çš„ system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯ lozes å¤§æ¨¡å‹è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯ lojzes å¤§æ¨¡å‹å­¦å‘˜ï¼Œæˆ‘å« lojzes\"),\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°ï¼Ÿ\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597b789-e241-4ff4-815e-a1e5f12081d4",
   "metadata": {},
   "source": [
    "### 1.1.4 æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e1a00d-cf34-4172-b4ad-b6fc63e92d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ï¼ğŸ˜Š\n",
      "\n",
      "æˆ‘æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æ¨¡å‹ï¼Œè™½ç„¶ä¸æ”¯æŒå¤šæ¨¡æ€è¯†åˆ«åŠŸèƒ½ï¼Œä½†æˆ‘æœ‰æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œå¯ä»¥å¸®ä½ å¤„ç†å›¾åƒã€txtã€pdfã€pptã€wordã€excelç­‰æ–‡ä»¶ï¼Œå¹¶ä»ä¸­è¯»å–æ–‡å­—ä¿¡æ¯è¿›è¡Œåˆ†æå¤„ç†ã€‚æˆ‘å®Œå…¨å…è´¹ä½¿ç”¨ï¼Œæ‹¥æœ‰128Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿˜æ”¯æŒè”ç½‘æœç´¢åŠŸèƒ½ï¼ˆéœ€è¦ä½ åœ¨Web/Appä¸­æ‰‹åŠ¨ç‚¹å¼€è”ç½‘æœç´¢æŒ‰é”®ï¼‰ã€‚\n",
      "\n",
      "ä½ å¯ä»¥é€šè¿‡å®˜æ–¹åº”ç”¨å•†åº—ä¸‹è½½æˆ‘çš„Appæ¥ä½¿ç”¨ã€‚æˆ‘å¾ˆä¹æ„å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€å¤„ç†æ–‡æ¡£ã€è¿›è¡Œå¯¹è¯äº¤æµç­‰ç­‰ï¼\n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯æ—¥å¸¸é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆæ„¿æ„ä¸ºä½ æä¾›æ”¯æŒï¼âœ¨"
     ]
    }
   ],
   "source": [
    "for token in model.stream(\"ä½ æ˜¯è°\"):\n",
    "    print(token.content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ef7df-6ff4-4082-8131-5df91eaaa8cd",
   "metadata": {},
   "source": [
    "### é›†æˆåƒé—® ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f884f754-ad95-4e6a-9c07-15dab9c284a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©ä½ å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€æä¾›ä¿¡æ¯æŸ¥è¯¢æœåŠ¡ï¼Œè¿˜èƒ½é™ªä½ èŠå¤©ã€å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•éœ€è¦å¸®åŠ©çš„åœ°æ–¹ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatLLM = ChatOpenAI(\n",
    "    api_key=\"sk-66bc27a6330f434f8751f8172a73064f\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # æ­¤å¤„ä»¥qwen-plusä¸ºä¾‹ï¼Œæ‚¨å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚æ¨¡å‹åˆ—è¡¨ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    # other params...\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"}]\n",
    "response = chatLLM.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7a6f0-70f3-4be6-892b-bf588442fe37",
   "metadata": {},
   "source": [
    "### é›†æˆåƒé—® DashScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f8544c-c048-46f9-a53b-b56025f84c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-community\n",
    "# pip install dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73fed25f-ab1b-4ca8-807c-ce2d59e39b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat resp: Hello\n",
      "chat resp: ! How can\n",
      "chat resp:  I assist you\n",
      "chat resp:  today?\n",
      "chat resp:  ğŸ˜Š\n",
      "chat resp: \n",
      "chat resp: \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chatLLM = ChatTongyi(\n",
    "    api_key=\"sk-66bc27a6330f434f8751f8172a73064f\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",   # æ­¤å¤„ä»¥qwen-plusä¸ºä¾‹ï¼Œæ‚¨å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚æ¨¡å‹åˆ—è¡¨ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    streaming=True,\n",
    ")\n",
    "res = chatLLM.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "for r in res:\n",
    "    print(\"chat resp:\", r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7ceb8-8a2e-4631-8c54-5737a8284e31",
   "metadata": {},
   "source": [
    "###  Ollama æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02265d3f-5530-4865-abab-bc30e49832cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db8e11-6190-4c51-9ae8-64dacb7b2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\"ä½ æ˜¯ä¸ªä¸“ä¸šçš„ç¿»è¯‘å®¶ï¼Œå¯ä»¥å°†ç”¨æˆ·çš„ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\"æˆ‘å–œæ¬¢ç¼–ç¨‹\"\n",
    "    )\n",
    "]\n",
    "\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58096eeb-4dd5-4b5a-93ae-3bf3c695a805",
   "metadata": {},
   "source": [
    "## 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8bcce-5aa0-4154-a748-12b537ebe87b",
   "metadata": {},
   "source": [
    "![](./img/2.png)\n",
    "\n",
    "### 1.2.1 Prompt æ¨¡æ¿å°è£…\n",
    "### 1. PromptTemplate å¯ä»¥ åœ¨æ¨¡æ¿ä¸­å®šä¹‰å˜é‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5798c49-6462-445b-9acf-4c7e9f83d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] input_types={} partial_variables={} template='ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "===Prompt===\n",
      "ç»™æˆ‘è®²ä¸€ä¸ªå…³äºå°ç±³çš„ç¬‘è¯\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸€ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='å°ç±³'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1730a6-2462-4b5e-98f6-af8bab688fe6",
   "metadata": {},
   "source": [
    "### 2. ChatPromptTemplate ç”¨æ¨¡æ¿è¡¨ç¤ºçš„å¯¹è¯ä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b7b8ff1-5630-49ef-8bde-2d64baa34489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯lojzeså¤§æ¨¡å‹çš„å®¢æœåŠ©æ‰‹,ä½ çš„åå­—å«å°lojzes', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={})]\n",
      "å—¨ï¼Œæˆ‘æ˜¯å°lojzesï¼Œæ˜¯lojzeså¤§æ¨¡å‹çš„å®¢æœåŠ©æ‰‹~å¾ˆé«˜å…´è®¤è¯†ä½ ï¼æˆ‘åœ¨è¿™é‡Œå¯ä»¥å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œé™ªä½ èŠå¤©ï¼Œæˆ–è€…ä¸€èµ·æ¢ç´¢æœ‰è¶£çš„è¯é¢˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ(â€¢Ì€á´—â€¢Ì)Ùˆ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import(\n",
    "    ChatPromptTemplate,HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatLLM = ChatOpenAI(\n",
    "    api_key=\"sk-66bc27a6330f434f8751f8172a73064f\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # æ­¤å¤„ä»¥qwen-plusä¸ºä¾‹ï¼Œæ‚¨å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚æ¨¡å‹åˆ—è¡¨ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹,ä½ çš„åå­—å«{name}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    product=\"lojzeså¤§æ¨¡å‹\",\n",
    "    name=\"å°lojzes\",\n",
    "    query=\"ä½ æ˜¯è°\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "response = chatLLM.invoke(prompt)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb27fd2-b97e-44d0-9ad3-0cfbfa7af54a",
   "metadata": {},
   "source": [
    "### 3. MessagesPlaceHolder æŠŠå¤šè½®å¯¹è¯å˜æˆæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635a878f-b50c-4e78-a7db-7d1522cc2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='who is Elon Musk?', additional_kwargs={}, response_metadata={}), AIMessage(content='Elon Musk is a billionaire entrepreneur,inventor,and industry designer', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate your answer to ä¸­æ–‡.', additional_kwargs={}, response_metadata={})]\n",
      "åŸƒéš†Â·é©¬æ–¯å…‹æ˜¯ä¸€ä½äº¿ä¸‡å¯Œç¿ä¼ä¸šå®¶ã€å‘æ˜å®¶å’Œå·¥ä¸šè®¾è®¡å¸ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template= HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [MessagesPlaceholder(\"history\"),human_message_template]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"who is Elon Musk?\")\n",
    "ai_message = AIMessage(content=\"Elon Musk is a billionaire entrepreneur,inventor,and industry designer\")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "     history=[human_message,ai_message],language=\"ä¸­æ–‡\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())\n",
    "\n",
    "\n",
    "result = chatLLM.invoke(messages)\n",
    "print(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f0a112-6179-43da-81e6-7478f075edb4",
   "metadata": {},
   "source": [
    "### 1.2.2 ä»æ–‡ä»¶åŠ è½½Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2dfb96e-9ea4-4f9f-9151-a5c7abe20534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Template====\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'\n",
      "====Prompt====\n",
      "ä¸¾ä¸€ä¸ªå…³äºå°æ˜çš„ä¾‹å­\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file('./prompt_template.txt',encoding=\"utf-8\")\n",
    "print(\"====Template====\")\n",
    "print(template)\n",
    "print(\"====Prompt====\")\n",
    "print(template.format(topic=\"å°æ˜\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951221a4-48a9-46cf-9675-0be3ff221b83",
   "metadata": {},
   "source": [
    "## 1.3 ç»“æ„åŒ–è¾“å‡º\n",
    "### 1.3.1 ç›´æ¥è¾“å‡ºPydamic å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31763834-8a29-4d3c-a45d-d30fabcad1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date(year=2025, month=12, day=11, ear=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel ,Field\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºç­”æ¡ˆ\n",
    "\n",
    "class Date(BaseModel):\n",
    "    year:int = Field(description=\"Year\")\n",
    "    month:int = Field(description=\"Month\")\n",
    "    day:int = Field(description=\"Day\")\n",
    "    ear:int = Field(description=\"BC or AD\")\n",
    "\n",
    "\n",
    "from langchain_core.prompts import(\n",
    "PromptTemplate,HumanMessagePromptTemplate\n",
    ") \n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "llm = init_chat_model('deepseek-chat',model_provider='deepseek',\n",
    "        api_key= \"sk-fa1e532719fc46aa83166d15619bb76a\"\n",
    "    )\n",
    "\n",
    "# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹\n",
    "\n",
    "strurctured_llm = llm.with_structured_output(Date)\n",
    "\n",
    "template = \"\"\"\n",
    "æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = template\n",
    ")\n",
    "query=\"2025å¹´12æœˆ11æ—¥çš„å¤©æ°”æ™´\"\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "strurctured_llm.invoke(input_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dce0a6-e4eb-4b5b-b55a-27d2d6bef5ef",
   "metadata": {},
   "source": [
    "### 1.3.2 è¾“å‡ºæŒ‡å®šçš„JSONæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ed4900-3ab5-42ac-bce6-d3064d10392e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 2025, 'month': 12, 'day': 11, 'era': 'AD'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## OpenAI æ¨¡å‹çš„JSONæ ¼å¼ \n",
    "## æ¯ä¸ªæ¨¡å‹çš„JSON æ ¼å¼ä¸ä¸€æ ·\n",
    "json_schema = {\n",
    "    \"title\": \"Date\",\n",
    "    \"description\": \"Formated date expression\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"year, YYYY\",\n",
    "        },\n",
    "        \"month\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"month, MM\",\n",
    "        },\n",
    "        \"day\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"day, DD\",\n",
    "        },\n",
    "        \"era\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"BC or AD\",\n",
    "        },\n",
    "   },\n",
    "}\n",
    "\n",
    "strurctured_llm = llm.with_structured_output(json_schema)\n",
    "strurctured_llm.invoke(input_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97611041-b288-4c55-a07c-e3cafb6359b4",
   "metadata": {},
   "source": [
    "### 1.3.3 ä½¿ç”¨OutputParse\n",
    "OutputParser å¯ä»¥æŒ‰æŒ‡å®šæ ¼å¼è§£ææ¨¡å‹çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dd53d44-cc23-4616-bb52-0b314b931e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "{\"year\": 2025, \"month\": 12, \"day\": 11, \"ear\": 1}\n",
      "\n",
      " è§£æåï¼š\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'year': 2025, 'month': 12, 'day': 11, 'ear': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "llm = init_chat_model('deepseek-chat',model_provider='deepseek',\n",
    "        api_key= \"sk-fa1e532719fc46aa83166d15619bb76a\"\n",
    "    )\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Date)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\\nç”¨æˆ·è¾“å…¥:{query}\\n{format_instructions}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\" + output.content)\n",
    "print(\"\\n è§£æåï¼š\")\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c382-9851-4f5b-8403-745169c34b88",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥ç”¨ PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22f49dea-6990-4c0e-8dc4-8402eb028a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "{\"year\": 2025, \"month\": 12, \"day\": 11, \"ear\": 1}\n",
      "\n",
      " è§£æåï¼š\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2025, month=12, day=11, ear=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "llm = init_chat_model('deepseek-chat',model_provider='deepseek',\n",
    "        api_key= \"sk-fa1e532719fc46aa83166d15619bb76a\"\n",
    "    )\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\" + output.content)\n",
    "print(\"\\n è§£æåï¼š\")\n",
    "parser.invoke(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72dee5a-2d56-4577-b2d5-b1c48ac2c6b3",
   "metadata": {},
   "source": [
    "OutputFixingParser åˆ©ç”¨å¤§æ¨¡å‹åšæ ¼å¼è‡ªåŠ¨çº é”™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e87cc323-f190-41b8-841a-b5dfc0476391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PydanticOutputParser:\n",
      "Invalid json output: {\"year\": 2025, \"month\": 12, \"day\": åä¸€, \"ear\": 1}\n",
      "For troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE \n",
      "OutputFixingParser:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2025, month=12, day=11, ear=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_classic.output_parsers import OutputFixingParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# çº é”™èƒ½åŠ›ä¸å¤§æ¨¡å‹èƒ½åŠ›ç›¸å…³\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser,llm=llm)\n",
    "\n",
    "bad_output = output.content.replace(\"11\",\"åä¸€\")\n",
    "print(\"PydanticOutputParser:\")\n",
    "try:\n",
    "    parser.invoke(bad_output)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"OutputFixingParser:\")\n",
    "new_parser.invoke(bad_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcbb7b-48cb-4af6-a963-d41a363e66e6",
   "metadata": {},
   "source": [
    "# Funcion Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a14f283b-167b-4589-9c51-d83f728803ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"multiply\",\n",
      "        \"args\": {\n",
      "            \"a\": 3.5,\n",
      "            \"b\": 4\n",
      "        },\n",
      "        \"id\": \"call_00_eGQCMClIsz9soFjH4y3E4t76\",\n",
      "        \"type\": \"tool_call\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool  \n",
    "\n",
    "@tool  \n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "     \n",
    "      Args: a:First integer \n",
    "            b:Second integer I\n",
    "     \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool  \n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "        Args: \n",
    "            a:First integer \n",
    "            b:Second integer\n",
    "    \"\"\"\n",
    "    \n",
    "    return a * b\n",
    "\n",
    "import json\n",
    "from langchain_core.messages import(\n",
    "    AIMessage, # ç­‰ä»·äº OpenAI æ¥å£ä¸­çš„ assisant role\n",
    "    HumanMessage, # ç­‰ä»·äº OpenAI æ¥å£ä¸­çš„ user role\n",
    "    SystemMessage # ç­‰ä»·äº OpenAI æ¥å£ä¸­çš„ system role\n",
    ")\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([add,multiply])\n",
    "query = \"3.5çš„ 4 å€æ˜¯å¤šå°‘ï¼Ÿ\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "output = llm_with_tools.invoke(messages)\n",
    "print(json.dumps(output.tool_calls,indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbb928-6d4f-4857-8bea-a747757986bd",
   "metadata": {},
   "source": [
    "å›ä¼  Function Call çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf4b5886-d566-45d4-a2f4-f9c3abe083a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"content\": \"3.5çš„ 4 å€æ˜¯å¤šå°‘ï¼Ÿ\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"human\",\n",
      "    \"name\": null,\n",
      "    \"id\": null\n",
      "}\n",
      "{\n",
      "    \"content\": \"æˆ‘æ¥å¸®æ‚¨è®¡ç®— 3.5 çš„ 4 å€æ˜¯å¤šå°‘ã€‚\",\n",
      "    \"additional_kwargs\": {\n",
      "        \"refusal\": null\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "        \"token_usage\": {\n",
      "            \"completion_tokens\": 76,\n",
      "            \"prompt_tokens\": 402,\n",
      "            \"total_tokens\": 478,\n",
      "            \"completion_tokens_details\": null,\n",
      "            \"prompt_tokens_details\": {\n",
      "                \"audio_tokens\": null,\n",
      "                \"cached_tokens\": 384\n",
      "            },\n",
      "            \"prompt_cache_hit_tokens\": 384,\n",
      "            \"prompt_cache_miss_tokens\": 18\n",
      "        },\n",
      "        \"model_provider\": \"deepseek\",\n",
      "        \"model_name\": \"deepseek-chat\",\n",
      "        \"system_fingerprint\": \"fp_eaab8d114b_prod0820_fp8_kvcache\",\n",
      "        \"id\": \"ae8983f0-7e6e-40bb-ad14-f639260d5e8a\",\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"name\": null,\n",
      "    \"id\": \"lc_run--019b0dcb-9632-7d91-abde-e3d3166f4382-0\",\n",
      "    \"tool_calls\": [\n",
      "        {\n",
      "            \"name\": \"multiply\",\n",
      "            \"args\": {\n",
      "                \"a\": 3.5,\n",
      "                \"b\": 4\n",
      "            },\n",
      "            \"id\": \"call_00_eGQCMClIsz9soFjH4y3E4t76\",\n",
      "            \"type\": \"tool_call\"\n",
      "        }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "        \"input_tokens\": 402,\n",
      "        \"output_tokens\": 76,\n",
      "        \"total_tokens\": 478,\n",
      "        \"input_token_details\": {\n",
      "            \"cache_read\": 384\n",
      "        },\n",
      "        \"output_token_details\": {}\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"content\": \"14.0\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"tool\",\n",
      "    \"name\": \"multiply\",\n",
      "    \"id\": null,\n",
      "    \"tool_call_id\": \"call_00_eGQCMClIsz9soFjH4y3E4t76\",\n",
      "    \"artifact\": null,\n",
      "    \"status\": \"success\"\n",
      "}\n",
      "{\n",
      "    \"content\": \"æˆ‘æ¥å¸®æ‚¨è®¡ç®— 3.5 çš„ 4 å€æ˜¯å¤šå°‘ã€‚\",\n",
      "    \"additional_kwargs\": {\n",
      "        \"refusal\": null\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "        \"token_usage\": {\n",
      "            \"completion_tokens\": 76,\n",
      "            \"prompt_tokens\": 402,\n",
      "            \"total_tokens\": 478,\n",
      "            \"completion_tokens_details\": null,\n",
      "            \"prompt_tokens_details\": {\n",
      "                \"audio_tokens\": null,\n",
      "                \"cached_tokens\": 384\n",
      "            },\n",
      "            \"prompt_cache_hit_tokens\": 384,\n",
      "            \"prompt_cache_miss_tokens\": 18\n",
      "        },\n",
      "        \"model_provider\": \"deepseek\",\n",
      "        \"model_name\": \"deepseek-chat\",\n",
      "        \"system_fingerprint\": \"fp_eaab8d114b_prod0820_fp8_kvcache\",\n",
      "        \"id\": \"ae8983f0-7e6e-40bb-ad14-f639260d5e8a\",\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"name\": null,\n",
      "    \"id\": \"lc_run--019b0dcb-9632-7d91-abde-e3d3166f4382-0\",\n",
      "    \"tool_calls\": [\n",
      "        {\n",
      "            \"name\": \"multiply\",\n",
      "            \"args\": {\n",
      "                \"a\": 3.5,\n",
      "                \"b\": 4\n",
      "            },\n",
      "            \"id\": \"call_00_eGQCMClIsz9soFjH4y3E4t76\",\n",
      "            \"type\": \"tool_call\"\n",
      "        }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "        \"input_tokens\": 402,\n",
      "        \"output_tokens\": 76,\n",
      "        \"total_tokens\": 478,\n",
      "        \"input_token_details\": {\n",
      "            \"cache_read\": 384\n",
      "        },\n",
      "        \"output_token_details\": {}\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"content\": \"14.0\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"tool\",\n",
      "    \"name\": \"multiply\",\n",
      "    \"id\": null,\n",
      "    \"tool_call_id\": \"call_00_eGQCMClIsz9soFjH4y3E4t76\",\n",
      "    \"artifact\": null,\n",
      "    \"status\": \"success\"\n",
      "}\n",
      "3.5 çš„ 4 å€æ˜¯ **14**ã€‚\n"
     ]
    }
   ],
   "source": [
    "messages.append(output)\n",
    "\n",
    "available_tools = {\"add\":add,\"multiply\":multiply}\n",
    "\n",
    "for tool_call in output.tool_calls:\n",
    "    selected_tool = available_tools[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "new_output = llm_with_tools.invoke(messages)\n",
    "for message in messages:\n",
    "    print(json.dumps(message.model_dump(),indent=4,ensure_ascii=False))\n",
    "    \n",
    "print(new_output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04d2bf-517c-47eb-b8bd-b6737da51d05",
   "metadata": {},
   "source": [
    "# 1.5 å°ç»“\n",
    "\n",
    "1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§  \n",
    "2. LangChain æä¾›äº† PromptTemplate ç±», å¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿  \n",
    "3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡  \n",
    "4. LangChain æä¾›äº† Function Calling çš„å°è£…  \n",
    "5. ä¸Šè¿°æ¨¡å‹å±äºLangChainä¸­è¾ƒä¸ºå®ç”¨çš„éƒ¨åˆ†\n",
    "\n",
    "![](./img/3.png)\n",
    "\n",
    "![](./img/4.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 æ–‡æ¡£åŠ è½½å™¨: DocumentLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "322dcd17-2a28-49ee-a651-f6080af7a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain-community pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dbd0371-8ad2-488a-8fb0-c4e837a815be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph æ ¸å¿ƒç»„ä»¶: Graphs, State,\n",
      "Nodes, Edges\n",
      " \n",
      "LangGraph æ ¸å¿ƒç»„ä»¶: Graphs, State, Nodes, Edges\n",
      "LangGraph\n",
      "ä¸»è¦åŠŸèƒ½\n",
      "LangGraph å¹³å°\n",
      "Graph(å›¾)\n",
      "StateGraph\n",
      "Compiling your graph(ç¼–è¯‘ä½ çš„å›¾)\n",
      "State(çŠ¶æ€)\n",
      "Nodes(èŠ‚ç‚¹)\n",
      "START èŠ‚ç‚¹\n",
      "END èŠ‚ç‚¹\n",
      "Edges(è¾¹)\n",
      "æ™®é€šè¾¹\n",
      "æ¡ä»¶è¾¹\n",
      "å…¥å£ç‚¹\n",
      "æ¡ä»¶å…¥å£ç‚¹\n",
      "Â \n",
      "LangGraph\n",
      " \n",
      "ğŸ¦œğŸ•¸ LangGraph âš¡ ä»¥å›¾çš„æ–¹å¼æ„å»ºè¯­è¨€ä»£ç† âš¡\n",
      "å®˜æ–¹æ–‡æ¡£åœ°å€ï¼šhttps://langchain-ai.github.io/langgraph/\n",
      "LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„åº“ï¼Œåˆ©ç”¨ LLM åˆ›å»ºä»£ç†å’Œå¤šä»£ç†å·¥ä½œæµã€‚ä¸\n",
      "å…¶ä»– LLM æ¡†æ¶ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼šå¾ªç¯æ€§ã€å¯æ§æ€§å’ŒæŒä¹…æ€§ã€‚LangGraph å…è®¸æ‚¨å®šä¹‰æ¶‰åŠ\n",
      "å¾ªç¯çš„æµç¨‹ï¼Œè¿™å¯¹äºå¤§å¤šæ•°ä»£ç†æ¶æ„è‡³å…³é‡è¦ï¼Œä½¿å…¶ä¸åŸºäº DAG çš„è§£å†³æ–¹æ¡ˆåŒºåˆ«å¼€æ¥ã€‚ä½œä¸ºä¸€ä¸ªéå¸¸åº•\n",
      "å±‚çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†å¯¹åº”ç”¨ç¨‹åºæµç¨‹å’ŒçŠ¶æ€çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œè¿™å¯¹äºåˆ›å»ºå¯é çš„ä»£ç†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œ\n",
      "LangGraph åŒ…å«å†…ç½®çš„æŒä¹…æ€§ï¼Œæ”¯æŒå…ˆè¿›çš„äººæœºåä½œå’Œè®°å¿†ç‰¹æ€§ã€‚\n",
      "LangGraph çš„çµæ„Ÿæ¥æºäº Pregel å’Œ Apache Beamã€‚å…¬å…±æ¥å£å— NetworkX çš„å¯å‘ã€‚LangGraph æ˜¯ç”± \n",
      "LangChain Inc å¼€å‘çš„ï¼Œå®ƒæ˜¯ LangChain çš„åˆ›å»ºè€…ï¼Œä½†å¯ä»¥åœ¨ä¸ä½¿ç”¨ LangChain çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n",
      "LangGraph å¹³å°æ˜¯ç”¨äºéƒ¨ç½² LangGraph ä»£ç†çš„åŸºç¡€è®¾æ–½ã€‚å®ƒæ˜¯ä¸€ä¸ªå•†ä¸šè§£å†³æ–¹æ¡ˆï¼Œç”¨äºå°†ä»£ç†åº”ç”¨ç¨‹\n",
      "åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæ„å»ºäºå¼€æºçš„ LangGraph æ¡†æ¶ä¹‹ä¸Šã€‚LangGraph å¹³å°ç”±å¤šä¸ªç»„ä»¶ç»„æˆï¼Œè¿™äº›ç»„ä»¶\n",
      "ååŒå·¥ä½œä»¥æ”¯æŒ LangGraph åº”ç”¨ç¨‹åºçš„å¼€å‘ã€éƒ¨ç½²ã€è°ƒè¯•å’Œç›‘æ§ï¼šLangGraph æœåŠ¡å™¨ï¼ˆAPIï¼‰ã€\n",
      "LangGraph SDKï¼ˆAPI å®¢æˆ·ç«¯ï¼‰ã€LangGraph CLIï¼ˆæ„å»ºæœåŠ¡å™¨çš„å‘½ä»¤è¡Œå·¥å…·ï¼‰ã€LangGraph \n",
      "Studioï¼ˆç”¨æˆ·ç•Œé¢/è°ƒè¯•å™¨ï¼‰ã€‚\n",
      "ä¸»è¦åŠŸèƒ½\n",
      " \n",
      "å¾ªç¯å’Œåˆ†æ”¯ï¼šåœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­å®ç°å¾ªç¯å’Œæ¡ä»¶è¯­å¥ã€‚\n",
      "æŒä¹…æ€§ï¼šåœ¨å›¾ä¸­çš„æ¯ä¸ªæ­¥éª¤ä¹‹åè‡ªåŠ¨ä¿å­˜çŠ¶æ€ã€‚åœ¨ä»»ä½•æ—¶å€™æš‚åœå’Œæ¢å¤å›¾æ‰§è¡Œä»¥æ”¯æŒé”™è¯¯æ¢å¤ã€\n",
      "â€œäººæœºäº¤äº’â€å·¥ä½œæµã€æ—¶é—´æ—…è¡Œç­‰ç­‰ã€‚\n",
      "â€œäººæœºäº¤äº’â€ï¼šä¸­æ–­å›¾æ‰§è¡Œä»¥æ‰¹å‡†æˆ–ç¼–è¾‘ä»£ç†è®¡åˆ’çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚\n",
      "æµæ”¯æŒï¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹äº§ç”Ÿè¾“å‡ºæ—¶æµå¼ä¼ è¾“è¾“å‡ºï¼ˆåŒ…æ‹¬ä»¤ç‰Œæµå¼ä¼ è¾“ï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader('./LangGraph.pdf')\n",
    "pages = loader.load_and_split()\n",
    "print(pages[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336b8d1-3bab-4a29-bcf8-4487eb01c7b6",
   "metadata": {},
   "source": [
    "## 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "### TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ab3d7de-72ae-4c75-80f7-9b48b30f47e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph æ ¸å¿ƒç»„ä»¶: Graphs, State,\n",
      "Nodes, Edges\n",
      " \n",
      "LangGraph æ ¸å¿ƒç»„ä»¶: Graphs, State, Nodes, Edges\n",
      "LangGraph\n",
      "ä¸»è¦åŠŸèƒ½\n",
      "LangGraph å¹³å°\n",
      "Graph(å›¾)\n",
      "StateGraph\n",
      "Compiling your graph(ç¼–è¯‘ä½ çš„å›¾)\n",
      "State(çŠ¶æ€)\n",
      "Nodes(èŠ‚ç‚¹)\n",
      "START èŠ‚ç‚¹\n",
      "END èŠ‚ç‚¹\n",
      "Edges(è¾¹)\n",
      "æ™®é€šè¾¹\n",
      "æ¡ä»¶è¾¹\n",
      "å…¥å£ç‚¹\n",
      "æ¡ä»¶å…¥å£ç‚¹\n",
      "Â \n",
      "LangGraph\n",
      " \n",
      "ğŸ¦œğŸ•¸ LangGraph âš¡ ä»¥å›¾çš„æ–¹å¼æ„å»ºè¯­è¨€ä»£ç† âš¡\n",
      "å®˜æ–¹æ–‡æ¡£åœ°å€ï¼šhttps://langchain-ai.github.io/langgraph/\n",
      "LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„åº“ï¼Œåˆ©ç”¨ LLM åˆ›å»ºä»£ç†å’Œå¤šä»£ç†å·¥ä½œæµã€‚ä¸\n",
      "å…¶ä»– LLM æ¡†æ¶ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼šå¾ªç¯æ€§ã€å¯æ§æ€§å’ŒæŒä¹…æ€§ã€‚LangGraph å…è®¸æ‚¨å®šä¹‰æ¶‰åŠ\n",
      "å¾ªç¯çš„æµç¨‹ï¼Œè¿™å¯¹äºå¤§å¤šæ•°ä»£ç†æ¶æ„è‡³å…³é‡è¦ï¼Œä½¿å…¶ä¸åŸºäº DAG çš„è§£å†³æ–¹æ¡ˆåŒºåˆ«å¼€æ¥ã€‚ä½œä¸ºä¸€ä¸ªéå¸¸åº•\n",
      "------------------------\n",
      "LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„åº“ï¼Œåˆ©ç”¨ LLM åˆ›å»ºä»£ç†å’Œå¤šä»£ç†å·¥ä½œæµã€‚ä¸\n",
      "å…¶ä»– LLM æ¡†æ¶ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼šå¾ªç¯æ€§ã€å¯æ§æ€§å’ŒæŒä¹…æ€§ã€‚LangGraph å…è®¸æ‚¨å®šä¹‰æ¶‰åŠ\n",
      "å¾ªç¯çš„æµç¨‹ï¼Œè¿™å¯¹äºå¤§å¤šæ•°ä»£ç†æ¶æ„è‡³å…³é‡è¦ï¼Œä½¿å…¶ä¸åŸºäº DAG çš„è§£å†³æ–¹æ¡ˆåŒºåˆ«å¼€æ¥ã€‚ä½œä¸ºä¸€ä¸ªéå¸¸åº•\n",
      "å±‚çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†å¯¹åº”ç”¨ç¨‹åºæµç¨‹å’ŒçŠ¶æ€çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œè¿™å¯¹äºåˆ›å»ºå¯é çš„ä»£ç†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œ\n",
      "LangGraph åŒ…å«å†…ç½®çš„æŒä¹…æ€§ï¼Œæ”¯æŒå…ˆè¿›çš„äººæœºåä½œå’Œè®°å¿†ç‰¹æ€§ã€‚\n",
      "LangGraph çš„çµæ„Ÿæ¥æºäº Pregel å’Œ Apache Beamã€‚å…¬å…±æ¥å£å— NetworkX çš„å¯å‘ã€‚LangGraph æ˜¯ç”± \n",
      "LangChain Inc å¼€å‘çš„ï¼Œå®ƒæ˜¯ LangChain çš„åˆ›å»ºè€…ï¼Œä½†å¯ä»¥åœ¨ä¸ä½¿ç”¨ LangChain çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n",
      "LangGraph å¹³å°æ˜¯ç”¨äºéƒ¨ç½² LangGraph ä»£ç†çš„åŸºç¡€è®¾æ–½ã€‚å®ƒæ˜¯ä¸€ä¸ªå•†ä¸šè§£å†³æ–¹æ¡ˆï¼Œç”¨äºå°†ä»£ç†åº”ç”¨ç¨‹\n",
      "åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæ„å»ºäºå¼€æºçš„ LangGraph æ¡†æ¶ä¹‹ä¸Šã€‚LangGraph å¹³å°ç”±å¤šä¸ªç»„ä»¶ç»„æˆï¼Œè¿™äº›ç»„ä»¶\n",
      "------------------------\n",
      "LangChain Inc å¼€å‘çš„ï¼Œå®ƒæ˜¯ LangChain çš„åˆ›å»ºè€…ï¼Œä½†å¯ä»¥åœ¨ä¸ä½¿ç”¨ LangChain çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n",
      "LangGraph å¹³å°æ˜¯ç”¨äºéƒ¨ç½² LangGraph ä»£ç†çš„åŸºç¡€è®¾æ–½ã€‚å®ƒæ˜¯ä¸€ä¸ªå•†ä¸šè§£å†³æ–¹æ¡ˆï¼Œç”¨äºå°†ä»£ç†åº”ç”¨ç¨‹\n",
      "åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæ„å»ºäºå¼€æºçš„ LangGraph æ¡†æ¶ä¹‹ä¸Šã€‚LangGraph å¹³å°ç”±å¤šä¸ªç»„ä»¶ç»„æˆï¼Œè¿™äº›ç»„ä»¶\n",
      "ååŒå·¥ä½œä»¥æ”¯æŒ LangGraph åº”ç”¨ç¨‹åºçš„å¼€å‘ã€éƒ¨ç½²ã€è°ƒè¯•å’Œç›‘æ§ï¼šLangGraph æœåŠ¡å™¨ï¼ˆAPIï¼‰ã€\n",
      "LangGraph SDKï¼ˆAPI å®¢æˆ·ç«¯ï¼‰ã€LangGraph CLIï¼ˆæ„å»ºæœåŠ¡å™¨çš„å‘½ä»¤è¡Œå·¥å…·ï¼‰ã€LangGraph \n",
      "Studioï¼ˆç”¨æˆ·ç•Œé¢/è°ƒè¯•å™¨ï¼‰ã€‚\n",
      "ä¸»è¦åŠŸèƒ½\n",
      " \n",
      "å¾ªç¯å’Œåˆ†æ”¯ï¼šåœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­å®ç°å¾ªç¯å’Œæ¡ä»¶è¯­å¥ã€‚\n",
      "æŒä¹…æ€§ï¼šåœ¨å›¾ä¸­çš„æ¯ä¸ªæ­¥éª¤ä¹‹åè‡ªåŠ¨ä¿å­˜çŠ¶æ€ã€‚åœ¨ä»»ä½•æ—¶å€™æš‚åœå’Œæ¢å¤å›¾æ‰§è¡Œä»¥æ”¯æŒé”™è¯¯æ¢å¤ã€\n",
      "â€œäººæœºäº¤äº’â€å·¥ä½œæµã€æ—¶é—´æ—…è¡Œç­‰ç­‰ã€‚\n",
      "â€œäººæœºäº¤äº’â€ï¼šä¸­æ–­å›¾æ‰§è¡Œä»¥æ‰¹å‡†æˆ–ç¼–è¾‘ä»£ç†è®¡åˆ’çš„ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚\n",
      "æµæ”¯æŒï¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹äº§ç”Ÿè¾“å‡ºæ—¶æµå¼ä¼ è¾“è¾“å‡ºï¼ˆåŒ…æ‹¬ä»¤ç‰Œæµå¼ä¼ è¾“ï¼‰ã€‚\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "\n",
    "\n",
    "for para in paragraphs:\n",
    "  print(para.page_content)\n",
    "  print('------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f767a05-769f-4763-9d7b-4241babfa066",
   "metadata": {},
   "source": [
    "## 2.3 å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09d87d8e-a13e-4c54-804a-96c3d432a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dashscope fasiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05496176-2bb8-4bcf-a557-528c38adac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph æ ¸å¿ƒç»„ä»¶: Graphs, State,\n",
      "Nodes, Edges\n",
      " \n",
      "LangGraph æ ¸å¿ƒç»„ä»¶: Graphs, State, Nodes, Edges\n",
      "LangGraph\n",
      "ä¸»è¦åŠŸèƒ½\n",
      "LangGraph å¹³å°\n",
      "Graph(å›¾)\n",
      "StateGraph\n",
      "Compiling your graph(ç¼–è¯‘ä½ çš„å›¾)\n",
      "State(çŠ¶æ€)\n",
      "Nodes(èŠ‚ç‚¹)\n",
      "START èŠ‚ç‚¹\n",
      "END èŠ‚ç‚¹\n",
      "Edges(è¾¹)\n",
      "æ™®é€šè¾¹\n",
      "æ¡ä»¶è¾¹\n",
      "å…¥å£ç‚¹\n",
      "æ¡ä»¶å…¥å£ç‚¹\n",
      "Â \n",
      "LangGraph\n",
      " \n",
      "ğŸ¦œğŸ•¸ LangGraph âš¡ ä»¥å›¾çš„æ–¹å¼æ„å»ºè¯­è¨€ä»£ç† âš¡\n",
      "å®˜æ–¹æ–‡æ¡£åœ°å€ï¼šhttps://langchain-ai.github.io/langgraph/\n",
      "LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„åº“ï¼Œåˆ©ç”¨ LLM åˆ›å»ºä»£ç†å’Œå¤šä»£ç†å·¥ä½œæµã€‚ä¸\n",
      "å…¶ä»– LLM æ¡†æ¶ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼šå¾ªç¯æ€§ã€å¯æ§æ€§å’ŒæŒä¹…æ€§ã€‚LangGraph å…è®¸æ‚¨å®šä¹‰æ¶‰åŠ\n",
      "å¾ªç¯çš„æµç¨‹ï¼Œè¿™å¯¹äºå¤§å¤šæ•°ä»£ç†æ¶æ„è‡³å…³é‡è¦ï¼Œä½¿å…¶ä¸åŸºäº DAG çš„è§£å†³æ–¹æ¡ˆåŒºåˆ«å¼€æ¥ã€‚ä½œä¸ºä¸€ä¸ªéå¸¸åº•\n",
      "-------\n",
      "LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„åº“ï¼Œåˆ©ç”¨ LLM åˆ›å»ºä»£ç†å’Œå¤šä»£ç†å·¥ä½œæµã€‚ä¸\n",
      "å…¶ä»– LLM æ¡†æ¶ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼šå¾ªç¯æ€§ã€å¯æ§æ€§å’ŒæŒä¹…æ€§ã€‚LangGraph å…è®¸æ‚¨å®šä¹‰æ¶‰åŠ\n",
      "å¾ªç¯çš„æµç¨‹ï¼Œè¿™å¯¹äºå¤§å¤šæ•°ä»£ç†æ¶æ„è‡³å…³é‡è¦ï¼Œä½¿å…¶ä¸åŸºäº DAG çš„è§£å†³æ–¹æ¡ˆåŒºåˆ«å¼€æ¥ã€‚ä½œä¸ºä¸€ä¸ªéå¸¸åº•\n",
      "å±‚çš„æ¡†æ¶ï¼Œå®ƒæä¾›äº†å¯¹åº”ç”¨ç¨‹åºæµç¨‹å’ŒçŠ¶æ€çš„ç»†ç²’åº¦æ§åˆ¶ï¼Œè¿™å¯¹äºåˆ›å»ºå¯é çš„ä»£ç†è‡³å…³é‡è¦ã€‚æ­¤å¤–ï¼Œ\n",
      "LangGraph åŒ…å«å†…ç½®çš„æŒä¹…æ€§ï¼Œæ”¯æŒå…ˆè¿›çš„äººæœºåä½œå’Œè®°å¿†ç‰¹æ€§ã€‚\n",
      "LangGraph çš„çµæ„Ÿæ¥æºäº Pregel å’Œ Apache Beamã€‚å…¬å…±æ¥å£å— NetworkX çš„å¯å‘ã€‚LangGraph æ˜¯ç”± \n",
      "LangChain Inc å¼€å‘çš„ï¼Œå®ƒæ˜¯ LangChain çš„åˆ›å»ºè€…ï¼Œä½†å¯ä»¥åœ¨ä¸ä½¿ç”¨ LangChain çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚\n",
      "LangGraph å¹³å°æ˜¯ç”¨äºéƒ¨ç½² LangGraph ä»£ç†çš„åŸºç¡€è®¾æ–½ã€‚å®ƒæ˜¯ä¸€ä¸ªå•†ä¸šè§£å†³æ–¹æ¡ˆï¼Œç”¨äºå°†ä»£ç†åº”ç”¨ç¨‹\n",
      "åºéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œæ„å»ºäºå¼€æºçš„ LangGraph æ¡†æ¶ä¹‹ä¸Šã€‚LangGraph å¹³å°ç”±å¤šä¸ªç»„ä»¶ç»„æˆï¼Œè¿™äº›ç»„ä»¶\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader('./LangGraph.pdf')\n",
    "pages = loader.load_and_split()\n",
    "# print(pages[0].page_content)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:1]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\",\n",
    "    dashscope_api_key=\"sk-66bc27a6330f434f8751f8172a73064f\"\n",
    ")\n",
    "index = FAISS.from_documents(texts,embeddings)\n",
    "# æ£€ç´¢top-5 \n",
    "retriver = index.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "docs = retriver.invoke(\"langGraph æ˜¯ä»€ä¹ˆ\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1228e6-ca4a-4d0f-8011-4791e47a58b4",
   "metadata": {},
   "source": [
    "# 3. Chain å’Œ LangChain Expression Language (LCEL)\n",
    "\n",
    "LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆChainã€‚LCELè‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œæ— éœ€ä»£ç æ›´æ”¹ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„LCEL Chainï¼‰ã€‚\n",
    "\n",
    "# LCEL çš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. æµæ”¯æŒï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä» LLM æµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚  \n",
    "2. å¼‚æ­¥æ”¯æŒï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚  \n",
    "3. ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚\n",
    "4. é‡è¯•å’Œå›é€€ï¼šä¸ºLCELé“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚  \n",
    "5. è®¿é—®ä¸­é—´ç»“æœï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚  \n",
    "6. è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚  \n",
    "7. æ— ç¼ LangSmith è·Ÿè¸ªé›†æˆï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚  \n",
    "8. æ— ç¼ LangServe éƒ¨ç½²é›†æˆï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚\n",
    "\n",
    "## 3.1 Pipeline å¼è°ƒç”¨ PromptTemplate ,LLM å’ŒOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af89d9db-27e6-45e9-bf98-9d872d2f955d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_classic.chat_models import init_chat_model\n",
    "from pydantic import BaseModel,Field\n",
    "from typing import List ,Dict,Optional\n",
    "from enum import Enum\n",
    "\n",
    "# ç»“æ„åŒ–è¾“å‡º\n",
    "class SortEnum(str,Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "class OrderingEnum(str,Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµæµªåŒ…åç§°\",default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\",default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\",default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\",default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\",default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\",default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(description=\"å‡åºæˆ–é™åºæ’åˆ—\",default=None)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"ä½ æ˜¯ä¸ªè¯­ä¹‰è§£æå™¨ï¼Œä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆ JSONè¡¨ç¤ºã€‚ä¸è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜\"),\n",
    "        (\"human\",\"{text}\")\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "llm = init_chat_model('deepseek-chat',model_provider='deepseek',\n",
    "        api_key= \"sk-fa1e532719fc46aa83166d15619bb76a\"\n",
    "    )\n",
    "\n",
    "strurctured_llm = llm.with_structured_output(Semantics)\n",
    "\n",
    "# LCEL è¡¨è¾¾å¼\n",
    "runnable = (\n",
    "    {\n",
    "        \"text\":RunnablePassthrough()\n",
    "    } | prompt | strurctured_llm\n",
    ")\n",
    "\n",
    "# è¿è¡Œ\n",
    "res = runnable.invoke(\"ä¸è¶…è¿‡ 100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›ï¼Ÿ\")\n",
    "\n",
    "print(\n",
    "    json.dumps(\n",
    "        res.model_dump(),indent=4,ensure_ascii=False\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68891e5-969c-48fe-89f9-7ed96fd9274d",
   "metadata": {},
   "source": [
    "## 3.2 ç”¨ LCEL å®ç°RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f5629b-81ef-47aa-8806-2259107ebe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒLangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„å¤šå‚ä¸è€…åº”ç”¨ç¨‹åºçš„åº“ï¼Œå®ƒåˆ©ç”¨ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ¥åˆ›å»ºä»£ç†å’Œå¤šä»£ç†å·¥ä½œæµã€‚"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_classic.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "loader = PyMuPDFLoader('./LangGraph.pdf')\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:1]]\n",
    ")\n",
    "llm = init_chat_model('deepseek-chat',model_provider='deepseek',\n",
    "        api_key= \"sk-fa1e532719fc46aa83166d15619bb76a\"\n",
    "    )\n",
    "# çŒåº“\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\",\n",
    "    dashscope_api_key=\"sk-66bc27a6330f434f8751f8172a73064f\"\n",
    ")\n",
    "index = FAISS.from_documents(texts,embeddings)\n",
    "# æ£€ç´¢top-5\n",
    "retriver = index.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "template = \"\"\" Answer the question base only on the following context:\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "#Chain\n",
    "rag_chain = (\n",
    "    {\"question\":RunnablePassthrough(),\"context\":retriver}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# rag_chain.invoke(\"LangGraph æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "# æµå¼è¾“å‡º\n",
    "resp = rag_chain.stream(\"LangGraph æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "for r in resp:\n",
    "    print(r,end='',flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b5fa6-113f-4e64-8683-ef040565a3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
